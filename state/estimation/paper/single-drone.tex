\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multicol,caption}
\usepackage{color}
\newcommand\tdoo[1]{ * \textcolor{red}{#1}}
\newcommand{\cmt}[1]{}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage[square,numbers]{natbib}
\geometry{margin=1in}
\usepackage[export]{adjustbox}

\title{Learning Switching Dynamical Models for Abnormality Detection in Autonomous Quad-copters}

\author{Mohammad Rahmani
\\mohammad.rahmani@aau.at
\\Networked and Embedded Systems 
\\Klagenfurt University 
\\ Austria
\and 
Berhard Rinner
\\bernhard.rinner@aau.at
\\Networked and Embedded Systems 
\\Klagenfurt University 
\\ Austria}

\begin{document}
\bibliographystyle{ieeetr}
\maketitle
\begin{abstract}
In this paper,  we extend a Switching Dynamical Linear System (SDLS) based on a multi level Dynamic Bayesian Network (DBN) from 4-dimensional state spaces to state spaces of arbitrary dimensions. The optimized number of discrete variables of the SDLS which form the discrete level of the DBN is determined by Growing Neural Gas (GNG). Additionally, a dynamic method is used to determine the neighborhoods of these discrete variables where quasi-static motions are valid. We examine its efficiency with three abnormality indicators for 6-dimensional training data derived from a simulated quad-copter by using three deviated scenario from a reference scenario according to which the DBN is trained. 
\end{abstract}
\begin{multicols}{2}

\section{Introduction}\label{sec:introduction}
    A self-aware (SA) robot can become the subject of its own attention such that it can process its own sensory data to understand experiences through which it goes  \cite{morin-2006-levels-of-consciousness-and-self-awareness-a-comparison-and-integration-of-various-neurocognitive-views}. In recent years, to improve autonomy and scalability of artificial systems, SA have been implemented in a vast variety of areas from self-aware cameras \cite{rinner-2015-self-aware-and-self-expressive-camera-networks} to robotics \cite{winfield-2014-robots-with-internal-models-a-route-to-self-aware-and-hence-safer-robots}. Inference and anomaly detection are two important capabilities of any self-aware robot \cite{regazzoni-2020-multi-sensorial-generative-and-descriptive-self-awareness-models-for-autonomous-systems}. Inference provides the robots  with the ability to predict their future states \cite{seth-2013-interoceptive-inference-emotion-and-the-embodied-self} while abnormality detection makes them capable of noticing the difference between their observations and predictions \cite{apps-2014-the-free-energy-self-a-predictive-coding-account-of-self-recognition}. Hence, robots will be capable of detecting new experiences to improve their inference models \cite{friston-2010-the-free-energy-principle-a-unified-brain-theory}. 
    
    This paper proposes a modeling, inference and abnormality detection for an autonomous robot following the ideas of self-awareness.
    Figure~\ref{fig:mjpf} presents an overview of this approach to estimate the current state of a robot and derive abnormality indicators. During offline learning, the robot follows a reference trajectory multiple times to generate positional data by its GPS sensor. Using Growing Neural Gas (GNG), the derived data is partitioned into clusters from which a set of regions with quasi-static velocities are derived. These clusters are used to learn the probabilistic parameters of a Dynamic Bayesian Network (DBN). During online prediction and abnormality detection, this DBN is used together with the current sensor measurements to compute abnormality signals which indicate whether the currently observed behavior is matching the predicted behavior from the DBN.  
    
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.45]{assets/offline-online-mjpf.jpeg}
        \caption{The proposed solution consist of two phases. The offline learning phase uses a sequence of measurements from a reference scenario to learn the necessary probabilistic parameters of a DBN. The online phase exploits the learned probabilistic DBN models to estimate the state and generate abnormality signals. In the DBN, the horizontal arrows present temporal causalities while the vertical arrows present causalities between the continuous and discrete levels}
        \label{fig:mjpf}
    \end{figure*} 
    
    This work improves a DBN-based modeling of a single robot behavior proposed in \cite{baydoun-2018-learning-switching-models-for-abnormality-detection-for-autonomous-driving} and assesses its inference and abnormality detection performance. These improvements are as follows. Firstly, to better limit noisier sensory data, dynamic neighborhoods in which quasi-static velocities are valid are considered according to the covariance matrix of members of each cluster. Furthermore, Mahalanobis distance is used to cluster measurements and compute the likelihood of the variables at different levels of the proposed DBN. Additionally, Growing Neural Gas (GNG) is used to dynamically determine the optimal number of clusters needed at discreet level. Finally, it has extended the model to any dimension and tested it by 6-dimensional data derived from a simulated quad-copter. 
    
    The reminder of this paper is organized as follows: Section~\ref{sec:related-work} discusses related work. Section~\ref{sec:learning-phase} describes the offline phase of our approach, and Section~\ref{sec:online-processing} summarizes the online phase. Section~\ref{sec:experiments} presents the conducted simulation study, and Section~\ref{sec:conclusion} concludes the paper with a summary and outlook about future work.
    
\section{Related Work} \label{sec:related-work}
    Dynamic Bayesian Networks (DBNs) \cite{murphy-2002-dynamic-bayesian-networks-representation-inference-and-learning} are data driven models which can be used as models to infer robot's future states and detect abnormalities. They are hierarchical models which use Bayesian filtering methods \cite{chen-2003-bayesian-filtering-from-kalman-filters-to-particle-filters-and-beyond} at different levels of their hierarchy to predict, estimate the current state and detect abnormalities according to the most recent sensory measurements. They build temporal relations between the states according to the data derived from an already known experience and consequently, during online testing, they update the posterior probability of robot states based on this measurement. Switching Dynamical Linear System (SDLS)\cite{sudderth-2009-nonparametric-bayesian-learning-of-switching-linear-dynamical-systems,doucet-2001-particle-filters-for-state-estimation-of-jump-markov-linear-systems,doucet-2001-rao-blackwellised-particle-filtering-for-dynamic-bayesian-networks} are special DBNs using which a dynamic non-linear model could be broken into sequential, linear models. They use switching, discrete random variables (super states) associated with these linear dynamic models at higher levels of the DBN to recursively estimate the joint posterior of state and the switching variables. Markov Jump Particle Filter (MJPF) \cite{baydoun-2018-learning-switching-models-for-abnormality-detection-for-autonomous-driving} is one such filtering system which uses 2-Time Slice DBN \cite{koller-2001-sampling-in-factored-dynamic-systems} SDLS for inference and abnormality detection at lower continuous and higher discrete level of its hierarchy. At continuous level, it uses a bank of Kalman Filters  \cite{kalman-1960-a-new-approach-to-linear-filtering-and-prediction-problems} which are associated with linear dynamic models derived from super states as Kalman Filter is only applicable in linear dynamic systems. These super states are corresponded to particles of a Particle Filter \cite{gordon-1993-novel-approach-to-nonlinear-non-gaussian-bayesian-state-estimation} at discrete level from which the aforementioned Kalman Filters get the control input of the dynamic system using which they predict the next state. 
    
    
\section{Offline learning}\label{sec:learning-phase}
    \paragraph{State space definition.} Since the goal in this paper is to estimate the state using regions of state space in which quasi-static velocities are valid, we consider the state space as 
    \begin{equation}
        X_t = [x_{1},\dots,x_{n},\dot{x}_{1},\dots,\dot{x}_{n}]^T
        \label{eq:state-space}
    \end{equation}
    where $x_1,\dots,x_n$ represent 0th-order derivatives (position) and $\dot{x}_1,\dots,\dot{x}_n$ represent corresponding first-order derivatives (velocities) at time step $t$. To accomplish this goal, let $p_t=[p_1,\dots,p_n]$ be the noisy measurement for the 0th-order derivatives of a state of a dynamical system. We define 
    \begin{equation}
        Z = \{ D[p_t, \frac{p_t-p_{t-1}}{\Delta t}]^T \ | \ t\in \{1,\dots,|Z|\}\}
        \label{eq:measurement-space}
    \end{equation}
    as a sequence weighted measurements where $\Delta t$ is the sampling time and 
    \begin{equation}
        D = 
        \begin{pmatrix}
        \alpha I_n & 0_n \\
        0_n & \beta I_n
        \end{pmatrix}
        \label{eq:weighting-matrix}
    \end{equation}
    such that $\beta > \alpha$. This multiplies the 0th derivative part (position) of the each member of $Z$ by $\alpha$ and the 1st derivative part (velocities) by $\beta$. Such multiplication causes the clustering algorithm that we apply later on $Z$ to prefer changes in velocities to cluster $Z$ rather than positions. 

    \paragraph{Learning discrete variable.} To cluster the weighted measurements $Z$, we use Growing Neural Gas (GNG) \cite{fritzke-1995-a-growing-neural-gas-network-learns-topologies}  which is an incremental clustering algorithm that can be used to cluster vectors without pre-defining the number of clusters in advance. Additionally, parameters of this algorithm such as local and global and scaling errors are just needed to be set once before running it. GNG detects a set of nodes, $\Psi = \{\Psi_1,...,\Psi_{|S|}\}$, in state space from which we measure the Mahalanobis distance for each weighted measurement $d(Z_i,\Psi_j)$. represents Mahalanobis distance between the weighted measurement $Z_i$ and the node $\Psi_j$, then $Z_i$ belongs to the cluster with the minimum distance $d(Z_i, \Psi_j)$ of all nodes. As such we cluster $Z$ into $|S|$ clusters.
    \begin{equation}
        S=\{S_1,\dots,S_{|S|}\}
        \label{eq:super-states}
    \end{equation}
    Each of these clusters can be presented by the mean and covariance matrix of the measurements they include. Each mean and covariance matrix pair $(\mu_i,\Sigma_i)$ is referred as a super state. The super state $S_j$ is said to be activated if the measurement $Z_i$ is a member of the neighborhood
    \begin{equation}
        C(\mu_j, 3\sqrt{tr(\Sigma_j)})
    \end{equation}
    such that $\mu_j$ is its center and $3\sqrt{tr(\Sigma_j)}$ is its radius where $tr(.)$ is the trace operator of $\Sigma_j$. We refer to these neighborhoods as known regions. The radius of these neighborhoods are dynamic since they vary depending on clusters' members. Regions outside these neighborhoods  are referred as unknown regions. Defining known regions using dynamic radius in comparison to static regions in \cite{baydoun-2018-learning-switching-models-for-abnormality-detection-for-autonomous-driving} limits better noisier measurements in the probabilistic parameters calculations we discuss later.\tdoo{}

    \paragraph{Learning super states transition matrices.} Each two consecutive members of $Z$ activate two consecutive super states of $S$. Counting consecutive activated super states for all members of $Z$ gives a probability transition matrix such as:
    \begin{equation}
        \pi_{SS} = \{p(S^i_{t-1} \ | \ S^j_{t},s) \ | \ i,j \in \{1,\dots,|S|\}\}
        \label{eq:superstate-transition-matrix}
    \end{equation}
    to predict the probability of staying in a known region or moving to another known region conditioned on knowing the previous super state after $s$ time steps.
        
    \paragraph{Process model at continuous level} Each $\mu_i$ for each super state, similar to each member of $Z$ in Equation \ref{eq:measurement-space} is formed of position components and corresponding velocities. The velocity part of each $\mu_i$ can be used as the control input in a process model in order to predict the next continuous state based on the current state. Mathematically, we can define the process model in continuous state space as follows 
        \begin{equation}
            X_{t} = FX_{t-1}+BU_{S_{t-1}}+\nu_{t-1}
            \label{eq:process-model}
        \end{equation}
        where 
        \begin{equation}
            F = 
            \begin{pmatrix}
            I_n & 0_n \\
            0_n & 0_n 
            \end{pmatrix}
        \end{equation}
        is the transition matrix. $B = [I_n \Delta t \ I_n]^T $ is the control matrix where $\Delta t$ is sampling time, $U_{S_{t-1}}=[I_n \ 0_n]\mu_{S_{t-1}}$. The process noise $\nu_{t-1}$ is drawn from a multi variate normal distribution such that $\nu_{t-1} \sim \mathcal{N}(0,\Sigma_{S_t})$. Whenever the previous state doesn't match any known region, $U_{S_{t-1}}$ is set to $0_{n,1}$. Similar to Equation~\ref{eq:superstate-transition-matrix}, we can define a set of probabilistic models such that
        \begin{equation}
            \pi_{XX} = \{p(X_t|X_{t-1},S^i_{t-1})\} \ | \ i \in \{1,\dots,|S|\} \}
            \label{eq:process-model}
        \end{equation}
        
        \paragraph{The likelihood of continuous states belong to a super state} Furthermore, the likelihood of continuous states belonging to a super states can be defined as 
        \begin{equation}
            \lambda_{XS}=\{p(X_t|S_{t})\} \ | \ i \in \{1,\dots,|S|\}\}
        \end{equation}  
        where 
        \begin{equation}
            p(X_t|S_{t}) = \frac{d(X_t,\mu_t)}{\sum_{i=1}^{|S|}d(X_t,\mu_i)}.
        \end{equation}  
    
\section{Online processing}\label{sec:online-processing}
    Having $\pi_{SS}$, $\pi_{XX}$ and $\lambda_{XS}$ computed using the methods described in previous section, with given set of measurements, we can extend MJPF to any dimension with the  definition of known regions. This model makes inferences using the learned DBN (cp.~Figure~\ref{fig:mjpf}). It uses a Particle Filter which is coupled with a bank of Kalman Filters at the continuous and discrete levels jointly for estimation. At the continuous level, states are inferred from measurements and  $\pi_{XX}$ predictions are computed by the Kalman Filters corresponded to each region. At the discrete level, $\pi_{SS}$ is used by the Particle Filter for future estimations. These two levels are related to each other by using the means of the super states to which particles correspond as the control inputs in each Kalman Filter.  
    
    \paragraph{Particle Filter} The Particle Filter uses a set of weighted vectors in the state space to estimate the posterior of an unknown probability distribution. Intuitively, these weights present the possibility of having solutions close to these corresponding states according to the most recent measurement and a given importance function, $q$, from which we sample instead of the unknown probability distribution. Since a sequential importance resampling (SIR) is used for the Particle Filter in this work, the importance function is taken as $q=p(S_t|S_{t-1})$. To estimate the posterior of $X_t$ based on the most recent measurement $Z_t$, let $V = \{v_{t}^{(i)}=(X_{t}^{(i)},w_{t}^{(i)}) \ | \ i \in \{1, \dots, |V|\}\}$ be the set of particles at time step $t$  where $X_{t}^{(i)}$ is the particle's associated state at the continuous state space, and $w_{t}^{(i)}$ is its corresponding weight. The associated state of each particle $v_{t}^{(i)}$ is either within a known region or in an unknown region. The symbol we use hereafter for such region is $S_{t}^{(i)}$.  
    
    \paragraph{Particle movement between regions} According to most recent measurement, we consider three different transition probabilities for a particle between regions $v_{t}^{(i)}=(X_{t}^{(i)},w_{t}^{(i)})$. When a particle has moved from one known region to another known region, the probability is $q$ multiplied by the probability that it stays in the same region after a given amount of time. When it  has moved from a known region to an unknown region, then it is $1-p(S_t|S_{t-1})$ where $S_t$ is the region with its mean closest to the particle's state. Finally, when a particle in an unknown region moves to a known region with $\mu_j$ and $\Sigma_j$, then the probability is taken as the maximum of $\{0, 1-\frac{d(X_t^*,\mu_j)}{\mu_j+3\sqrt{tr(\Sigma_j)}}\}$.
    
    \paragraph{Computing the weight of the particles} To compute the weight of the particles when new measurements become available $Z_t$, we need to compute three probabilities. The first is 
    \begin{equation}
        p(Z_t|X_{t}^{(j)}) = \frac{d(Z_t,X_{t}^{(j)})}{\sum_{i=1}^{|V|}d(Z_t,X_{t}^{(i)})}
    \end{equation}
    The second is
    \begin{equation}
        p(X_{t}^{(j)}|S_{t}^{(j)}) = \frac{d(X_{t}^{(j)},\mu_{t}^{(j)})}{\sum_{i=1}^{|V|}d(X_{t}^{(i)},\mu_{t}^{(j)})}
    \end{equation}
    where $\mu_{t}^{(i)}$ is the mean corresponded to super state $S_{t}^{(i)}$. 
    The last probability is $p(X_t^{(i)}|X_{t-1}^{(i)}(S_{t-1}^{(i)}))$ in which $X_{t-1}^{(i)}(S_{t-1}^{(i)})$ means the probability depends on the super state $S_{t-1}^{(i)}$. This probability is computed by using the the Kalman Filter corresponding to  super state $(S_{t-1}^{(i)})$ with control input $U_{S_{t-1}}$ in Equation $\ref{eq:process-model}$.  As such, the weights of particles are approximated as 
    \begin{equation}
        \small
        W_t = W_{t-1}\sum_{i=1}^{|V|} p(Z_t|X_t^{(i)})p(X_t^{(i)}|S_t^{(i)})p(X_t^{(i)}|X_{t-1}^{(i)}(S_{t-1}^{(i)}))
    \end{equation}
    where $W_{t-1}$ is 
    \begin{equation}
        W_{t-1} = exp^{-(db1+db2)}
    \end{equation}
    such that
    \begin{equation}
        \small
        db1=-\ln \sum_{i=1}^{|V|} \sqrt{p(X_t^{(i)}|S_t^{(i)})p(X_t^{(i)}|X_{t-1}^{(i)}(S_{t-1}^{(i)}))}
    \end{equation}
    and
    \begin{equation}
        \small
        db2 =  -\ln \sum_{i=1}^{|V|} \sqrt{p(Z_t|X_t^{(i)})p(X_t^{(i)}|X_{t-1}^{(i)}(S_{t-1}^{(i)}))}
    \end{equation}
    
    
    \paragraph{} $db1$ and $db2$ are the Bhattacharyya distances \cite{bhattacharyya-1943-on-a-measure-of-divergence-between-two-statistical-populations-defined-by-probability-distributions} between $p(X^{(i)}_{t}|X^{(i)}_{t-1}(S^{(i)}_{t-1}))$, the particle's state prediction, and $p(X_{t}^{(i)}|S_{t}^{(i)})$ which is the probability of particle being inside a predicted region and $p(Z_{t}|X_{t}^{(i)})$ which measures how much the particle is likely according to measurement $Z_{t}$. 
       
    \paragraph{Abnormality indicators} As it implies from the definition of $db1$ and $db2$, they are good indicators to detect deviations from a learned model. $db1$ presents the distance between the predicted super state and the probability of being in that super state. Intuitively, it indicates if particles are coherent with  the predicted super state of the learned model. Large values of this indicator show that the current experience is outside the known regions. In other words larger values denote measurements outside the domain of known regions. Simply put, it presents the distance between the predicted super state and the probability of being in that super state. This indicator considers only the known region in which the agent is and ignores its velocity vector. On the contrary, $db2$ can be attributed to the similarity between the state prediction and the continuous state evidence related to the new measurement in each super state. That is, it denotes the difference between the expected prediction and the likelihood behavior.  Finally, since the Kalman Innovation is defined as 
    \begin{equation}
        g = d(Z_t,H\hat{X}_{t|t-1})
    \end{equation}
    where $H$ is the measurement matrix and $\hat{X}_{t|t-1}$ is the prediction of state at $t$ given the state at $t-1$, it can be used as an another abnormality indicator. Similar to $db2$, it presents the distance between the predicted state and current measurement.  As such in any experiment, we expect these two values to approximate each other. 
        
        
\section{Experiments}\label{sec:experiments}
    In this section, the robot's DBN model learns the parameters mentioned in Section~\ref{sec:learning-phase} from a sequence of measurements generated from a GPS sensor of a simulated quad-copter (see Figure~\ref{fig:gps-rectangle}) while following a  reference scenario multiple times (see Figure~\ref{fig:rectangle-points}). In the online processing, the quad-copter follows deviated scenarios from the aforementioned scenario and generates the abnormality signals described in Section~\ref{sec:online-processing} to assess quad-copters ability in detecting deviations from learned scenarios. To generate the aforementioned measurement sequence, we deploy the CTU-MRS simulation framework \cite{baca-2020-the-mrs-uav-system-pushing-the-frontiers-of-reproducible-research-real-world-deployment-and-education-with-autonomous-unmanned-aerial-vehicles}. This framework uses Robot Operating System (ROS) \cite{quigley-2009-ros-an-open-source-robot-operating-system} to control quad-copters and Gazebo \cite{koenig-2004-design-and-use-paradigms-for-gazebo-an-open-source-multi-robot-simulator} to graphically realize the simulations. In each of the following experiments, a set of points of the reference scenario (input trajectory) is fed to a ROS service of a quad-copter realized in this framework. The quad-copter then follows these points using an autopilot based on Model Predictive Control (MPC) \cite{garcia-1989-model-predictive-control-theory-and-practice-a-survey}. As such, it is possible to record the values generated by a ROS topic that outputs GPS sensor data (observed trajectory).  These positions are 3-dimensional vectors such as $P_t=[x_{1},x_{2},x_{3}]^T$ at time step $t$. The GPS sensor noise is assumed to be zero mean Gaussian. The covariance matrix of sensor's noise is considered to be equal in all regions to $0.127\times I_6$. We take $\alpha = 1$ and $\beta=20$ in Equation \ref{eq:weighting-matrix} to build members of $Z$ similar to definition of Equation \ref{eq:measurement-space}. 
    
    \paragraph{Scenario 1, reference scenario, Threshold values} In this scenario, a quad-copter is requested to follow a square trajectory as illustrated in Figure~\ref{fig:rectangle-points} $10$ times with constant velocity magnitude. The autopilot completes each cycle in $4000$ time steps and generates a total of $40000$ position data points which are used to learn the DBN parameters as discussed in Section~\ref{sec:learning-phase}. After pre-processing, the resulting sequence of weighted measurements are fed to a GNG for clustering. In this experiment the following GNG parameter setting is used: maximum number of nodes: $100$; maximum age for edges: $100$; maximum number of iterations: $3000$; local error rate: $0.5$; global error rate: $0.0005$; and scaling amount: $0.5$. These values are chosen according to \cite{holmstr-2002-examensarbete-dv-3-2002-0830-growing-neural-gas-experiments-with-gng-gng-with-utility-and-supervised-gng,iqbal-2019-clustering-optimization-for-abnormality-detection-in-semi-autonomous-systems} which have conducted studies on the optimal settings for GNG. 
    
    \paragraph{} Figure \ref{fig:gng-rectancle-100-clusters} presents the clusters of the measurements from this experiment such that each color presents a different cluster. Feeding the learned DBN with the same sequence of measurements, we can compute abnormality values for all three indicators using which it is possible to define threshold values for each of them by taking their mean plus their variance as threshold. In this experiment these values are $0.25$ for $db1$ and $0.32$ for $db2$ and the Kalman Innovation.  
    
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/rectangle-points.jpg}
        \caption{Points for quad-copter's MPC controller to follow scenario 1 trajectory}
        \label{fig:rectangle-points}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/gps-sensor-training.png}
        \caption{Measured GPS sensor data (observed trajectory) from a quad-copter following the input trajectory with an MPC-based autopilot.}
        \label{fig:gps-rectangle}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/gng-rectancle-100-clusters.png}
        \caption{GNG clusters from the data set illustrated in Figure \ref{fig:gps-rectangle}  }
        \label{fig:gng-rectancle-100-clusters}
    \end{figure*}
        
    \paragraph{Scenario 2, obstacle avoidance by horizontal turning} In this scenario, the quad-copter turns left in the middle of the first side of the square as illustrated in Figure~\ref{fig:turn-left-mpc-points} to avoid an imaginary obstacle using its autopilot's MPC controller. Considering this deviation from the learned model in scenario 1, abnormality values are expected to exceed the threshold values approximately from time steps $333$ to $1333$ which is the time interval that the deviation from the learned scenario (scenario 1) happens. Figure~\ref{fig:gps-sensor-turn-left} illustrates quad-copter's GPS sensors recordings while performing scenario 2.
    
    Figure~\ref{fig:abnomality-signals-100-clusters-turn-left-with-label} shows the abnormality values derived from the learned DBN model and the corresponding GPS data. As it is expected, since both $db2$ and Kalman Innovation measure the distance between the new measurement and the continuous state prediction inside each region, then we expect their values to be close to each other. Since the velocity vector in the middle of this zone is similar to reference scenario, both these indicators can predict the next states in this zone and hence the abnormality values drop below the threshold. Intuitively, $db1$ should indicate if particles are coherent with the predicted region. Since turning left falls outside the known regions of learned experience, $db1$ generates abnormality values larger than threshold. This value is the largest in the middle of this zone since the largest distance between the known regions from the reference scenario and unknown regions in this scenario occur in this section.
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/turn-left-mpc-points.png}
        \caption{Points for quad-copter's MPC controller to follow scenario 2 trajectory}
        \label{fig:turn-left-mpc-points}
    \end{figure*}
    
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/gps-sensor-turn-left.png}
        \caption{Measured GPS sensor data (observed trajectory) from a quad-copter following the input trajectory from scenario 2 with an MPC-based autopilot.}
        \label{fig:gps-sensor-turn-left}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[width=\textwidth,center]{assets/abnomality-signals-100-clusters-turn-left-with-label.png}
        \caption{Abnormality values generated from DB1, DB2  and Kalman Innovation while realizing scenario 2 trajectory}
        \label{fig:abnomality-signals-100-clusters-turn-left-with-label}
    \end{figure*}
    
    \paragraph{Scenario 3, Turn left, fly up}  In this scenario, the quad-copter performs two different maneuvers to avoid two obstacles on the opposite sides of the square in the reference scenario. On the side to which the quad-copter reaches first, the quad-copter turns left around the obstacle and on the opposite side, the quad-copter increases height to avoid it. As such, abnormalities are expected to happen  between $333$ to $1333$ and $3000$ and $4000$ time steps accordingly. Figure \ref{fig:turn-left-fly-up-mpc-points} illustrates how quad-copters autopilot is requested to behave and Figure \ref{fig:turn-left-fly-up-gps-points} illustrates its corresponded GPS recorded values. 
    
    Figure~\ref{fig:abnomality-signals-100-clusters-turn-left-fly-up} illustrates the abnormality values resulted by performing this scenario. Similar to presence of a single deviation (scenario 2) from scenario 1, in this case, presence of two deviations are detected by all three indicators. During flying up maneuver, the autopilot request for rotors' power increase in advance to reach the highest point at requested time step. As such, abnormality values higher than threshold appear earlier than time step $3000$. At time step $3666$, the auto pilot, tries to lose height while still keeping some horizontal velocity which results in observing high abnormalities even after time step $4000$. 
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/turn-left-fly-up-mpc-points.png}
        \caption{Points for quad-copter's MPC controller to follow scenario 3 trajectory}
        \label{fig:turn-left-fly-up-mpc-points}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/turn-left-fly-up-gps-points.png}
        \caption{Measured GPS sensor data (observed trajectory) from a quad-copter following the input trajectory from scenario 3 with an MPC-based autopilot.}
        \label{fig:turn-left-fly-up-gps-points}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[width=\textwidth,center]{assets/abnomality-signals-100-clusters-turn-left-fly-up-with-labels.png}
        \caption{Abnormality values generated from DB1, DB2  and Kalman Innovation while performing scenario 3}
        \label{fig:abnomality-signals-100-clusters-turn-left-fly-up}
    \end{figure*}
        
    
    \paragraph{Scenario 4, U-turn} In this scenario, the quad-copter is requested to return back to opposite direction when it faces two obstacles at the end of two opposite sides of the square in scenario 1. The requested maneuver is illustrated in Figure~\ref{fig:uturn-mpc-points}, and Figure~\ref{fig:uturn-gps} illustrates quad-copter's recorded GPS data. Making quad-copter's autopilot to follow such trajectory points, results in expecting abnormal signals higher than threshold values from time steps $1000$ to $2000$ and $4666$ to $5666$. As Figure~\ref{fig:abnomality-signals-100-clusters-u-turn} shows, $db2$ which presents the difference between the likelihood behavior and expected prediction and Kalman Innovation raise above the threshold from the beginning of the first interval and remains high until the end of the second interval. This is the result of the opposite velocity in comparison to what has been learned in scenario 1.  On the contrary, $db1$, raises only during the two expected time intervals. The reason that $db1$ doesn't raise above the threshold in regions with opposite velocity in comparison to the learned model in  scenario 1 is that the distance between the predicted continuous state and the probability by which the quad-copter falls in a super state is low since it is a region from which the quad-copter has crossed during the reference scenario. 
    
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/uturn-mpc-points.png}
        \caption{Points for quad-copter's MPC controller to follow scenario 4 trajectory}
        \label{fig:uturn-mpc-points}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[scale=0.5]{assets/uturn-gps-with-labels.png}
        \caption{Measured GPS sensor data (observed trajectory) from a quad-copter following the input trajectory from scenario 4 with an MPC-based autopilot.}
        \label{fig:uturn-gps}
    \end{figure*}
    \begin{figure*}[]
        \centering
        \includegraphics[width=\textwidth,center]{assets/abnomality-signals-100-clusters-u-turn-with-labels.png}
        \caption{Abnormality values generated from DB1, DB2  and Kalman Innovation while performing scenario 4}
        \label{fig:abnomality-signals-100-clusters-u-turn}
    \end{figure*}
    
    
    \section{Conclusion and future work}\label{sec:conclusion}
        In this paper we improved Markov Jump Particle Filter for state estimation and abnormality detection from 4 dimensional state spaces to state spaces of any dimension and we tested it by a moving quad-copter. GNG is used to determine the number of regions in which quasi-static velocities are valid. We proposed these regions as neighborhood with varying radius to limit contribution of nosier measurements in training the MJPF. Furthermore, instead of Euclidean distance, Mahalanobis was used to cluster the measurements and compute the likelihoods. As a future work, we will consider extending the aforementioned improvements to coupling two MJPFs learned from interaction of two quad-copters as two Coupled Hidden Markov Models \cite{rezek-2000-learning-interaction-dynamics-with-couple-hidden-markov-models} similar to what is proposed in \cite{baydoun-2019-prediction-of-multi-target-dynamics-using-discrete-descriptors-an-interactive-approach}. 
        


\bibliography{refs.bib}
\end{multicols}
\end{document}
